# mnist_vae
Variational Autoencoder using MNIST dataset

[Examples](#examples)  
[Description](#description)  
[TODO](#todo)  
[Links](#links)

## Examples  
| ![Exploring Latent Space gif](https://github.com/temirgaliyev/mnist_vae/blob/master/static/example.gif) | 
|:--:| 
| *Exploring Latent Space* |

| ![Some examples from test set](https://github.com/temirgaliyev/mnist_vae/blob/master/static/example.png) | 
|:--:| 
| *Exploring Latent Space* |

## Description
Another one Variational Autoencoder trained on MNIST

## TODO
- [x] Create VAE training pipeline and train
- [x] Change Model
  - [x] BCE loss instead of MSE
  - [x] Convolutional Pooling instead of MaxPool
- [x] Minimum working example in Colab
- [ ] Train VAE on different dataset (ex. [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html ))  
  
## Links
[Github: Pytorch Examples](https://github.com/pytorch/examples)  
[Arxiv: Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)  
